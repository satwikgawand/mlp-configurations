# Multi-Layer Perceptron Configurations


This repository is a collection of experimental Python notebooks where each one deals with a different hyperparameter and observes the effect different values have on the model's performance. The configuration for every variation is the same except for the parameter being experimented upon.

The experiment utilizes a baseline Sequential Multi-Layer Perceptron. The parameters being tested are:
- batch_size
- epochs
- learning_rate
- loss_functions
- network_architectures
- optimizers

## File Structure:
- mlp-configurations (repo)
    - parameter
        - parameter.ipynb
        - figures
            - all the figures generated from the notebook