{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils.vis_utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the data_processing file to get the training and validation sets\n",
    "\n",
    "%run ../data_processing.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile parameters (can be updated for different variations and test)\n",
    "X_TRAIN = x_train\n",
    "Y_TRAIN = y_train\n",
    "\n",
    "X_VALID = x_valid\n",
    "Y_VALID = y_valid\n",
    "\n",
    "X_TEST = x_test\n",
    "Y_TEST = y_test\n",
    "\n",
    "VALIDATION = (x_valid, y_valid)\n",
    "\n",
    "ACTIVATION_FUNCTION = 'relu'\n",
    "ACTIVATION_FUNCTION_OUTPUT = 'softmax'\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "INPUT_DIM = 12\n",
    "KERNEL_INITIALIZER = 'he_uniform'\n",
    "LEARNING_RATE = 0.03\n",
    "LOSS_FUNCTION = 'categorical_crossentropy'\n",
    "METRICS = ['accuracy']\n",
    "VERBOSE = 0\n",
    "\n",
    "\n",
    "# list of optimizers to test (can be updated to test different optimizers)\n",
    "optimizers = {'SGD': keras.optimizers.SGD(learning_rate=LEARNING_RATE),\n",
    "              'RMSprop': keras.optimizers.RMSprop(learning_rate=LEARNING_RATE),\n",
    "              'Adam': keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "              'AdamW': keras.optimizers.AdamW(learning_rate=LEARNING_RATE),\n",
    "              'Adadelta': keras.optimizers.Adadelta(learning_rate=LEARNING_RATE),\n",
    "              'Adagrad': keras.optimizers.Adagrad(learning_rate=LEARNING_RATE),\n",
    "              'Adamax': keras.optimizers.Adamax(learning_rate=LEARNING_RATE),\n",
    "              'Adafactor': keras.optimizers.Adafactor(learning_rate=LEARNING_RATE),\n",
    "              'Nadam': keras.optimizers.Nadam(learning_rate=LEARNING_RATE),\n",
    "              'Ftrl': keras.optimizers.Ftrl(learning_rate=LEARNING_RATE),\n",
    "              }\n",
    "\n",
    "# dataframe to hold the loss and accuracy for each optimizer\n",
    "df_result = pd.DataFrame(columns=['optimizer', 'training_loss',\n",
    "                                  'training_accuracy', 'validation_loss',\n",
    "                                  'validation_accuracy', 'test_loss',\n",
    "                                  'test_accuracy'])\n",
    "\n",
    "# clears the dataset to avoid redundancy\n",
    "df_result = df_result.iloc[0:0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a baseline classifier\n",
    "def create_baseline():\n",
    "\n",
    "    # init a sequential NN\n",
    "    classifier = Sequential()\n",
    "\n",
    "    # Define the model architecture\n",
    "    \n",
    "    # 1st layer - takes in input\n",
    "    classifier.add(Dense(units=512, kernel_initializer=KERNEL_INITIALIZER,\n",
    "                         activation=ACTIVATION_FUNCTION, input_dim=INPUT_DIM))\n",
    "    # 2nd layer\n",
    "    classifier.add(Dense(units=128, kernel_initializer=KERNEL_INITIALIZER,\n",
    "                         activation=ACTIVATION_FUNCTION))\n",
    "    # 3rd layer\n",
    "    classifier.add(Dense(units=96, kernel_initializer=KERNEL_INITIALIZER,\n",
    "                         activation=ACTIVATION_FUNCTION))\n",
    "    # output layer\n",
    "    classifier.add(Dense(units=2, kernel_initializer=KERNEL_INITIALIZER,\n",
    "                         activation=ACTIVATION_FUNCTION_OUTPUT))\n",
    "\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through the optimizers list to test all the optimizers\n",
    "for OPTIMIZER in optimizers:\n",
    "    \n",
    "    # create a baseline model\n",
    "    model = create_baseline()\n",
    "\n",
    "    # compile the model with values defined above\n",
    "    model.compile(optimizer=optimizers[OPTIMIZER],\n",
    "                  loss=LOSS_FUNCTION, metrics=METRICS)\n",
    "    \n",
    "    # train the model\n",
    "    history = model.fit(X_TRAIN, Y_TRAIN, validation_data=VALIDATION,\n",
    "                            epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
    "                            verbose=VERBOSE)\n",
    "\n",
    "    # plot\n",
    "    pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "    plt.grid = True\n",
    "    plt.ylim(0, 1)\n",
    "    plt.title('Optimizer: ' + str(OPTIMIZER))\n",
    "    plt.savefig('figures/optimizer_'+ OPTIMIZER +'.png')\n",
    "    plt.show()\n",
    "\n",
    "    # evaluation - loss and accuracy\n",
    "    model_train_loss, model_train_accuracy = model.evaluate(X_TRAIN, Y_TRAIN,\n",
    "                                                            verbose=VERBOSE)\n",
    "    model_valid_loss, model_valid_accuracy = model.evaluate(X_VALID, Y_VALID,\n",
    "                                                            verbose=VERBOSE)\n",
    "    model_test_loss, model_test_accuracy = model.evaluate(X_TEST, Y_TEST,\n",
    "                                                          verbose=VERBOSE)\n",
    "\n",
    "    # add the evaluation results to the dataset\n",
    "    df_result = pd.concat([pd.DataFrame([[OPTIMIZER,\n",
    "                                          model_train_loss,\n",
    "                                          model_train_accuracy,\n",
    "                                          model_valid_loss,\n",
    "                                          model_valid_accuracy,\n",
    "                                          model_test_loss,\n",
    "                                          model_test_accuracy]],\n",
    "                                          columns=df_result.columns),\n",
    "                                          df_result],\n",
    "                                          ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the evaluation results dataframe\n",
    "\n",
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize loss for training, validation and test sets\n",
    "\n",
    "# x values\n",
    "x = ['training_loss', 'validation_loss', 'test_loss']\n",
    "\n",
    "# training and validation loss for each optimizer\n",
    "for _idx in range(df_result.shape[0]):\n",
    "    plt.plot(x, [df_result.iloc[_idx]['training_loss'],\n",
    "                 df_result.iloc[_idx]['validation_loss'],\n",
    "                 df_result.iloc[_idx]['test_loss']],\n",
    "                 label=str(df_result.iloc[_idx]['optimizer']))\n",
    "\n",
    "# plot\n",
    "plt.grid = True\n",
    "plt.ylim(0, 1)\n",
    "plt.title('Model Loss with varying Optimizers')\n",
    "plt.legend(loc='upper right')\n",
    "plt.savefig('figures/loss.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize accuracy for training, validation and test sets\n",
    "\n",
    "# x values\n",
    "x = ['training_accuracy', 'validation_accuracy', 'test_accuracy']\n",
    "\n",
    "# training and validation accuracy for each optimizer\n",
    "for _idx in range(df_result.shape[0]):\n",
    "    plt.plot(x, [df_result.iloc[_idx]['training_accuracy'],\n",
    "                 df_result.iloc[_idx]['validation_accuracy'],\n",
    "                 df_result.iloc[_idx]['test_accuracy']],\n",
    "                 label=str(df_result.iloc[_idx]['optimizer']))\n",
    "\n",
    "# plot\n",
    "plt.grid = True\n",
    "plt.ylim(0, 1)\n",
    "plt.title('Model Accuracy with varying Optimizers')\n",
    "plt.legend(loc='upper right')\n",
    "plt.savefig('figures/accuracy.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
